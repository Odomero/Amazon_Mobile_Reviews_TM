---
title: "Topic Modeling"
author: "Joao Macosso, Odomero, Koki"
date: "2023-02-06"
output: 
  html_document:
    theme: spacelab
    highlight: tango
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: true
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
options(warn=-1)
suppressWarnings(expression)
knitr::opts_chunk$set(echo = TRUE)
```

# Required Libraries

```{r, message=FALSE}
library(readr)
library(dplyr)
library(pander)
library(tm)
library(SnowballC)
library(textstem)
library(magrittr)
library(tidytext)
library(tm)
library(topicmodels)
library(tidyverse)
library(scales)
library(caret)
library(e1071)
library(pROC)
library(glmnet)
```

## Import some user define functions

```{r}
source("src/SupportFunctions.R")
```

#NLP

### Read Required Datasets

```{r warning=FALSE , message=FALSE}
phone_review_data <- read_csv("data/mobile-phones-20191226-reviews.csv")
brands <- read_csv("data/mobile-phones-20191226-items.csv")

#Merge the dataset to include brands
phone_review_data <- phone_review_data %>% left_join(brands %>% select(asin,brand),
                                by = "asin")
# We keep only the rows with available brand
phone_review_data <- phone_review_data %>% filter(!is.na(brand))

```

The file is too large, it contains more than 30K reviews, rendering takes too much of the memory, Therefore only the 15% of it is will be used
```{r}
# The file is too large, rendering takes too much of the memory,
# Therefore only the 15% of it is will be used
set.seed(10)
selection <- createDataPartition(phone_review_data$rating, p=0.15, list=FALSE)
phone_review_data <- phone_review_data[selection,]
```


# Classification

Our aim is to predict the rating of a based on a given review comement by the user, we will se binary classifier, therefore, we treat reviews lower or equal 2 as 0(Bad review), reviews larger or equal 4 as 1, good reviews.

```{r}
phone_review_data = phone_review_data %>% filter(rating != 3) %>%
  mutate(rating = ifelse(rating <= 2, 0, 1))

# first put smstext in tm corpus format
corpus <- VCorpus(VectorSource(phone_review_data$body))
# standardize to lowercase
corpus <- tm_map(corpus, content_transformer(tolower))
# remove tm stopwords
corpus <- tm_map(corpus, removeWords, stopwords())
# standardize whitespaces
corpus <- tm_map(corpus, stripWhitespace)
# remove punctuation
corpus <- tm_map(corpus, removePunctuation)
#stem
corpus <- tm_map(corpus, stemDocument)

corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, removeWords, stopwords("spanish"))

```
## Get the term matrix

```{r}
dtm <- DocumentTermMatrix(corpus)
dtm
```

```{r}
frequent <- findFreqTerms(dtm, 10)
head(frequent,8)
```

## Clean the data
```{r}
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
corpus <-tm_map(corpus,toSpace,"\\d.*")
corpus <-tm_map(corpus,toSpace,"ll")
corpus <-tm_map(corpus,toSpace,"re")
corpus <-tm_map(corpus,toSpace,"ve")
corpus <- tm_map(corpus, stripWhitespace)

```

## Split the train test data

```{r}
set.seed(10)
# set for the original raw data 
train_idx <- createDataPartition(phone_review_data$rating, p=0.7, list=FALSE)
train1 <- phone_review_data[train_idx,]
test1 <- phone_review_data[-train_idx,]

# set for the cleaned-up data
train2 <- corpus[train_idx]
test2 <- corpus[-train_idx]
```

```{r}
dtm2 <- DocumentTermMatrix(corpus, list(global = c(2, Inf), dictionary = frequent))
inspect(dtm2)

dict2 <- findFreqTerms(dtm2, lowfreq=10)

```

```{r}
# the same apply on train and test sets
phone_train <- DocumentTermMatrix(train2, list(dictionary=dict2))
phone_test <- DocumentTermMatrix(test2, list(dictionary=dict2))
```

```{r}
# compare number of terms
dtm
dtm2
phone_train
phone_test
```

```{r}
convert_counts <- function(x) {
  x <- ifelse(x > 0, 1, 0)
}

phone_train <- phone_train %>% apply(MARGIN=2, FUN=convert_counts)
phone_test <- phone_test %>% apply(MARGIN=2, FUN=convert_counts)

# make it back to a data frame
phone_train <- as.data.frame(phone_train)
phone_test <- as.data.frame(phone_test)

```

```{r}
str(phone_train)
```

```{r}
# prepare the data
phone_train1 <- cbind(label=factor(train1$rating), phone_train)
phone_test1 <- cbind(label=factor(test1$rating), phone_test)

phone_train1 <-as.data.frame(phone_train1)
phone_test1 <-as.data.frame(phone_test1)
```

## Modeling

### Logistic regression


```{r}
phone_rating_model <- glm(label~.,
                          data=phone_train1,
                          family = "binomial")
```

```{r}
# Predict test data based on model
predict_reg <- predict(phone_rating_model, 
                       na.omit(phone_test1), type = "response")
   
# Changing probabilities
predict_reg <- ifelse(predict_reg >0.5, 1, 0)
   
# Evaluating model accuracy
# using confusion matrix
table(phone_test1$label, predict_reg, dnn=c("Actual", "Predicted"))
#summary(phone_rating_model)
```


```{r}

confMatrix1 <- confusionMatrix(factor(predict_reg), factor(phone_test1$label), positive = "1")
confMatrix1

```


```{r}
   
# ROC-AUC Curve

test_roc = roc(phone_test1$label ~ predict_reg, plot = TRUE, print.auc = TRUE)
as.numeric(test_roc$auc)
```
Our logistic regression did not converge, this is most likely due to large sparsity of the data and possibly multicollinearity, Let us try to address these issues by using logistic regression with penalizations, we are going to use Lasso for regularization

## Lasso


First lets convert the ata into matrix
```{r}
X <- phone_train1 %>% select(-label) %>% as.matrix()
Y <- as.matrix(phone_train1$label)
XTest <- as.matrix(phone_test1 %>% select(-label))
```


Generally, the purpose of regularization is to balance accuracy and simplicity. This means, a model with the smallest number of predictors that also gives a good accuracy. To this end, the function cv.glmnet() finds also the value of lambda that gives the simplest model but also lies within one standard error of the optimal value of lambda
```{r}
cv.lasso <- cv.glmnet(X, Y, alpha = 1, family = "binomial")
print(paste("Optimal lambda:", cv.lasso$lambda.min))

# Fit the final model on the training data
model_lasso <- glmnet(X, Y, alpha = 1, family = "binomial",
                lambda = cv.lasso$lambda.min)


```

```{r}
# Predict test data based on model
predict_lasso_reg <- predict(model_lasso, 
                             XTest,
                       type = "response")

# Changing probabilities
predict_lasso_reg <- ifelse(predict_lasso_reg >0.5, 1, 0)
   
# Evaluating model accuracy
# using confusion matrix
confMatrix1_lasso <- confusionMatrix(factor(predict_lasso_reg), factor(phone_test1$label), positive = "1")
confMatrix1_lasso
```
```{r}
# ROC-AUC Curve

test_roc_lasso = roc(phone_test1$label ~ as.vector(predict_lasso_reg), 
                     plot = TRUE, print.auc = TRUE)
as.numeric(test_roc_lasso$auc)
```

We can see that most metrix have improved a bit such as accuracy, AUC, while specifity is worse now.
Also noting that our data is highly unbalanced with most reviews having rating 4 or 5, so it is very difficult to get a good model, an alternative would be to balance the dataset.